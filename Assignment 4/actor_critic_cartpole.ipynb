{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abqG_rJUyMU"
      },
      "source": [
        "# Actor Critic Method\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/13<br>\n",
        "**Last modified:** 2020/05/13<br>\n",
        "**Description:** Implement Actor Critic Method in CartPole environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1ssWwl4UyMd"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This script shows an implementation of Actor Critic method on CartPole-V0 environment.\n",
        "\n",
        "### Actor Critic Method\n",
        "\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to two possible outputs:\n",
        "\n",
        "1. Recommended action: A probability value for each action in the action space.\n",
        "   The part of the agent responsible for this output is called the **actor**.\n",
        "2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n",
        "   future. The part of the agent responsible for this output is the **critic**.\n",
        "\n",
        "Agent and Critic learn to perform their tasks, such that the recommended actions\n",
        "from the actor maximize the rewards.\n",
        "\n",
        "### CartPole-V0\n",
        "\n",
        "A pole is attached to a cart placed on a frictionless track. The agent has to apply\n",
        "force to move the cart. It is rewarded for every time step the pole\n",
        "remains upright. The agent, therefore, must learn to keep the pole from falling over.\n",
        "\n",
        "### References\n",
        "\n",
        "- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCWW7s2sUyMi"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnFgEwLQUyMl"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration parameters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v1\")  # Create the environment\n",
        "\n",
        "# env.seed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jle-SdbXUyMo"
      },
      "source": [
        "## Implement Actor Critic network\n",
        "\n",
        "This network learns two functions:\n",
        "\n",
        "1. Actor: This takes as input the state of our environment and returns a\n",
        "probability value for each action in its action space.\n",
        "2. Critic: This takes as input the state of our environment and returns\n",
        "an estimate of total rewards in the future.\n",
        "\n",
        "In our implementation, they share the initial layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybKpMAXvUyMq"
      },
      "source": [
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=[action, critic])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6Rd2hCUyMu"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXrINkk6UyMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1797cfdf-087f-4206-ad0e-447e3719006a"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "huber_loss = keras.losses.Huber()\n",
        "# action_probs_history = []\n",
        "# critic_value_history = []\n",
        "# rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "while True:  # Run until solved\n",
        "    action_probs_history = []\n",
        "    critic_value_history = []\n",
        "    rewards_history = []\n",
        "\n",
        "    # Corrected state initialization for gymnasium\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            # env.render(); # Adding this line would show the attempts\n",
        "\n",
        "            # Simplified and corrected state conversion\n",
        "            state_tensor = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            action_probs, critic_value = model(state_tensor, training=True) # training=True is fine\n",
        "\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            # Ensure action_probs is squeezed correctly for np.random.choice\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "\n",
        "            # CRITICAL: Clip action probabilities to prevent log(0)\n",
        "            action_probs_history.append(tf.math.log(tf.clip_by_value(action_probs[0, action], 1e-8, 1.0)))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Update state for next iteration\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize returns\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in zip(action_probs_history, critic_value_history, returns):\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
        "\n",
        "        # Backpropagation\n",
        "        # Sum actor and critic losses\n",
        "        total_loss = tf.reduce_sum(actor_losses) + tf.reduce_sum(critic_losses)\n",
        "\n",
        "        # Add gradient clipping for stability\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 5.0) # Clip gradients by global norm\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "    # if episode_count % 50 == 0:\n",
        "    #     print(f\"Episode {episode_count} | Reward: {episode_reward} | Action probs: {action_probs.numpy()} | Value: {critic_value.numpy()}\")\n",
        "\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 9.55 at episode 10\n",
            "running reward: 15.87 at episode 20\n",
            "running reward: 19.15 at episode 30\n",
            "running reward: 20.20 at episode 40\n",
            "running reward: 20.47 at episode 50\n",
            "running reward: 20.63 at episode 60\n",
            "running reward: 20.22 at episode 70\n",
            "running reward: 21.37 at episode 80\n",
            "running reward: 22.30 at episode 90\n",
            "running reward: 24.03 at episode 100\n",
            "running reward: 24.25 at episode 110\n",
            "running reward: 23.38 at episode 120\n",
            "running reward: 20.28 at episode 130\n",
            "running reward: 19.85 at episode 140\n",
            "running reward: 23.19 at episode 150\n",
            "running reward: 21.46 at episode 160\n",
            "running reward: 21.78 at episode 170\n",
            "running reward: 20.52 at episode 180\n",
            "running reward: 24.66 at episode 190\n",
            "running reward: 25.58 at episode 200\n",
            "running reward: 24.94 at episode 210\n",
            "running reward: 30.03 at episode 220\n",
            "running reward: 35.45 at episode 230\n",
            "running reward: 40.28 at episode 240\n",
            "running reward: 42.18 at episode 250\n",
            "running reward: 41.46 at episode 260\n",
            "running reward: 38.36 at episode 270\n",
            "running reward: 37.68 at episode 280\n",
            "running reward: 37.70 at episode 290\n",
            "running reward: 37.49 at episode 300\n",
            "running reward: 39.32 at episode 310\n",
            "running reward: 40.83 at episode 320\n",
            "running reward: 49.23 at episode 330\n",
            "running reward: 54.06 at episode 340\n",
            "running reward: 54.17 at episode 350\n",
            "running reward: 51.26 at episode 360\n",
            "running reward: 46.05 at episode 370\n",
            "running reward: 46.49 at episode 380\n",
            "running reward: 45.03 at episode 390\n",
            "running reward: 55.42 at episode 400\n",
            "running reward: 56.21 at episode 410\n",
            "running reward: 60.72 at episode 420\n",
            "running reward: 80.84 at episode 430\n",
            "running reward: 84.49 at episode 440\n",
            "running reward: 84.65 at episode 450\n",
            "running reward: 84.01 at episode 460\n",
            "running reward: 102.23 at episode 470\n",
            "running reward: 115.02 at episode 480\n",
            "running reward: 123.17 at episode 490\n",
            "running reward: 117.25 at episode 500\n",
            "running reward: 117.11 at episode 510\n",
            "running reward: 138.35 at episode 520\n",
            "running reward: 142.71 at episode 530\n",
            "running reward: 159.85 at episode 540\n",
            "running reward: 141.67 at episode 550\n",
            "running reward: 146.53 at episode 560\n",
            "running reward: 157.51 at episode 570\n",
            "running reward: 151.84 at episode 580\n",
            "running reward: 154.89 at episode 590\n",
            "running reward: 145.88 at episode 600\n",
            "running reward: 146.95 at episode 610\n",
            "running reward: 165.80 at episode 620\n",
            "running reward: 195.30 at episode 630\n",
            "Solved at episode 630!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouswvo6sUyND"
      },
      "source": [
        "## Visualizations\n",
        "In early stages of training:\n",
        "![Imgur](https://i.imgur.com/5gCs5kH.gif)\n",
        "\n",
        "In later stages of training:\n",
        "![Imgur](https://i.imgur.com/5ziiZUD.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduce Neurons 128 -> 20"
      ],
      "metadata": {
        "id": "2P_S_TWRggAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "# num_hidden = 128\n",
        "num_hidden = 20\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "huber_loss = keras.losses.Huber()\n",
        "# action_probs_history = []\n",
        "# critic_value_history = []\n",
        "# rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "\n",
        "while True:  # Run until solved\n",
        "    action_probs_history = []\n",
        "    critic_value_history = []\n",
        "    rewards_history = []\n",
        "\n",
        "    # Corrected state initialization for gymnasium\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            # env.render(); # Adding this line would show the attempts\n",
        "\n",
        "            # Simplified and corrected state conversion\n",
        "            state_tensor = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            action_probs, critic_value = model(state_tensor, training=True) # training=True is fine\n",
        "\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            # Ensure action_probs is squeezed correctly for np.random.choice\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "\n",
        "            # CRITICAL: Clip action probabilities to prevent log(0)\n",
        "            action_probs_history.append(tf.math.log(tf.clip_by_value(action_probs[0, action], 1e-8, 1.0)))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Update state for next iteration\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize returns\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in zip(action_probs_history, critic_value_history, returns):\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
        "\n",
        "        # Backpropagation\n",
        "        # Sum actor and critic losses\n",
        "        total_loss = tf.reduce_sum(actor_losses) + tf.reduce_sum(critic_losses)\n",
        "\n",
        "        # Add gradient clipping for stability\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 5.0) # Clip gradients by global norm\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "    # if episode_count % 50 == 0:\n",
        "    #     print(f\"Episode {episode_count} | Reward: {episode_reward} | Action probs: {action_probs.numpy()} | Value: {critic_value.numpy()}\")\n",
        "\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQvNSZFXOpcn",
        "outputId": "0db1a168-a146-4989-96a2-7ea985e0e2b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 72.67 at episode 10\n",
            "running reward: 131.25 at episode 20\n",
            "running reward: 181.07 at episode 30\n",
            "running reward: 174.21 at episode 40\n",
            "Solved at episode 49!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fa8S1sriOrRt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}